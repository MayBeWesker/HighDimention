{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e12bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2347278/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a934c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f13bf767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "class Options_AllenCahnHD:\n",
    "    def __init__(self):\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--cuda', default=True, help='if you use cuda')\n",
    "        \n",
    "        parser.add_argument('--bot_top', type=tuple, default=(-1, 1), help='a tuple of the form (bot, top)')\n",
    "        parser.add_argument('--T', type=float, default=1., help='a float T of time domain [0, T]')\n",
    "        parser.add_argument('--dim', default=50, help='dimension')\n",
    "        parser.add_argument('--N_r', default=2400, help='num of interior points')\n",
    "        parser.add_argument('--N_b', default=1200, help='num of boundary points')\n",
    "        parser.add_argument('--N_0', default=1200, help='num of initial points')\n",
    "        \n",
    "        parser.add_argument('--backbone_layers', type=list, default=([200]*3), help='list of nn layers of backbone')\n",
    "        parser.add_argument('--backbone_lr', type=float, default=5e-3, help='initial learning rate of backbone')\n",
    "        \n",
    "        parser.add_argument('--backbone_gamma', type=float, default=0.85, help='gamma in lr_scheduler for backbone optimizer')\n",
    "        parser.add_argument('--step_size', type=int, default=1000, help='step_size of lr_scheduler for Adam optimizer')\n",
    "        \n",
    "        parser.add_argument('--iters_Adam', type=int, default=20000, help='iters for stage 1 used Adam')\n",
    "        \n",
    "        parser.add_argument('--lam_res', type=float, default=1, help='weight of loss_res')\n",
    "        parser.add_argument('--lam_bcs', type=float, default=100, help='weight of loss_bcs')\n",
    "        parser.add_argument('--lam_ics', type=float, default=100, help='weight of loss_ics')\n",
    "        \n",
    "        self.parser = parser\n",
    "\n",
    "    def parse_default(self):\n",
    "        args = self.parser.parse_args(args=[])\n",
    "        args.device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "        \n",
    "        # 增加网络输入输出层\n",
    "        args.backbone_layers = [args.dim + 1] + args.backbone_layers + [1]\n",
    "        return args\n",
    "    \n",
    "\n",
    "args = Options_AllenCahnHD().parse_default()\n",
    "print(args.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4f2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(state, is_best=None, save_dir=None):\n",
    "    last_model = os.path.join(save_dir, 'last_model.pth')\n",
    "    torch.save(state, last_model)\n",
    "    if is_best:\n",
    "        best_model = os.path.join(save_dir, 'best_model.pth')\n",
    "        shutil.copyfile(last_model, best_model)\n",
    "\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "\n",
    "def grad(outputs, inputs):\n",
    "    \"\"\" compute the derivative of outputs associated with inputs\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "    outputs: (N, 1) tensor\n",
    "    inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs,\n",
    "                               grad_outputs=torch.ones_like(outputs),\n",
    "                               create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6e3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06429a",
   "metadata": {},
   "source": [
    "## 网络模型 (Modified ResNet / ResNet / Modified MLP / MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb86f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedResNet(nn.Module):\n",
    "    def __init__(self, mlp_layers):\n",
    "        super(ModifiedResNet, self).__init__()\n",
    "        \n",
    "        self.encoder_u = nn.Sequential()\n",
    "        self.encoder_u.add_module('fc_u', nn.Linear(mlp_layers[0], mlp_layers[1], bias=True))\n",
    "        self.encoder_u.add_module('act_u', nn.Tanh())\n",
    "        \n",
    "        self.encoder_v = nn.Sequential()\n",
    "        self.encoder_v.add_module('fc_v', nn.Linear(mlp_layers[0], mlp_layers[1], bias=True))\n",
    "        self.encoder_v.add_module('act_v', nn.Tanh())\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        \n",
    "        first_layer = nn.Sequential()\n",
    "        first_layer.add_module(f'fc0', nn.Linear(mlp_layers[0], mlp_layers[1], bias=True))\n",
    "        first_layer.add_module(f'act0', nn.Tanh())\n",
    "        self.model.add_module(f'first', first_layer)\n",
    "        \n",
    "        for i in range(1, len(mlp_layers)-2):\n",
    "            block = nn.Sequential()\n",
    "            block.add_module(f'fc{i}_0', nn.Linear(mlp_layers[i], mlp_layers[i+1], bias=True))\n",
    "            block.add_module(f'act{i}_0', nn.Tanh())\n",
    "            block.add_module(f'fc{i}_1', nn.Linear(mlp_layers[i], mlp_layers[i+1], bias=True))\n",
    "            block.add_module(f'act{i}_1', nn.Tanh())\n",
    "            self.model.add_module(f'block{i}', block)\n",
    "\n",
    "        last_layer = nn.Sequential()\n",
    "        last_layer.add_module(f'fc{len(mlp_layers)-2}', nn.Linear(mlp_layers[-2], mlp_layers[-1], bias=False))\n",
    "        self.model.add_module(f'last', last_layer)\n",
    "        \n",
    "#         for param in self.parameters():\n",
    "#             if len(param.shape) > 1:\n",
    "#                 nn.init.kaiming_normal_(param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        u = self.encoder_u(X)\n",
    "        v = self.encoder_v(X)\n",
    "        \n",
    "        X = self.model[0](X)\n",
    "        for i_block in range(1, len(self.model) - 1):\n",
    "            X_ = self.model[i_block](X)\n",
    "            X = X + X_\n",
    "            X = X / 2.\n",
    "            X = (1 - X) * u + X * v\n",
    "        return self.model[-1](X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3fcf7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, mlp_layers):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        \n",
    "        first_layer = nn.Sequential()\n",
    "        first_layer.add_module(f'fc0', nn.Linear(mlp_layers[0], mlp_layers[1], bias=True))\n",
    "        first_layer.add_module(f'act0', nn.Tanh())\n",
    "        self.model.add_module(f'first', first_layer)\n",
    "        \n",
    "        for i in range(1, len(mlp_layers)-2):\n",
    "            block = nn.Sequential()\n",
    "            block.add_module(f'fc{i}_0', nn.Linear(mlp_layers[i], mlp_layers[i+1], bias=True))\n",
    "            block.add_module(f'act{i}_0', nn.Tanh())\n",
    "            block.add_module(f'fc{i}_1', nn.Linear(mlp_layers[i], mlp_layers[i+1], bias=True))\n",
    "            block.add_module(f'act{i}_1', nn.Tanh())\n",
    "            self.model.add_module(f'block{i}', block)\n",
    "\n",
    "        last_layer = nn.Sequential()\n",
    "        last_layer.add_module(f'fc{len(mlp_layers)-2}', nn.Linear(mlp_layers[-2], mlp_layers[-1], bias=False))\n",
    "        self.model.add_module(f'last', last_layer)\n",
    "        \n",
    "#         for param in self.parameters():\n",
    "#             if len(param.shape) > 1:\n",
    "#                 nn.init.kaiming_normal_(param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.model[0](X)\n",
    "        for i_block in range(1, len(self.model) - 1):\n",
    "            X_ = self.model[i_block](X)\n",
    "            X = X_ + X\n",
    "        return self.model[-1](X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5393d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedMLP(nn.Module):\n",
    "    def __init__(self, mlp_layers):\n",
    "        super(ModifiedMLP, self).__init__()\n",
    "        \n",
    "        self.encoder_u = nn.Sequential()\n",
    "        self.encoder_u.add_module('fc_u', nn.Linear(mlp_layers[0], mlp_layers[1], bias=True))\n",
    "        self.encoder_u.add_module('act_u', nn.Tanh())\n",
    "        \n",
    "        self.encoder_v = nn.Sequential()\n",
    "        self.encoder_v.add_module('fc_v', nn.Linear(mlp_layers[0], mlp_layers[1], bias=True))\n",
    "        self.encoder_v.add_module('act_v', nn.Tanh())\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        for i in range(len(mlp_layers)-2):\n",
    "            layer = nn.Sequential()\n",
    "            layer.add_module(f'fc{i}', nn.Linear(mlp_layers[i], mlp_layers[i+1], bias=True))\n",
    "            layer.add_module(f'act{i}', nn.Tanh())\n",
    "            self.model.add_module(f'layer{i}', layer)\n",
    "\n",
    "        last_layer = nn.Sequential()\n",
    "        last_layer.add_module(f'fc{len(mlp_layers)-2}', nn.Linear(mlp_layers[-2], mlp_layers[-1], bias=False))\n",
    "        self.model.add_module(f'layer{len(mlp_layers)-2}', last_layer)\n",
    "        \n",
    "#         for param in self.parameters():\n",
    "#             if len(param.shape) > 1:\n",
    "#                 nn.init.kaiming_normal_(param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        u = self.encoder_u(X)\n",
    "        v = self.encoder_v(X)\n",
    "        \n",
    "        for i in range(len(self.model) - 1):\n",
    "            X = self.model[i](X)\n",
    "            X = X / 2.\n",
    "            X = (1 - X) * u + X * v\n",
    "        return self.model[-1](X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22ca867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, mlp_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        for i in range(len(mlp_layers)-2):\n",
    "            layer = nn.Sequential()\n",
    "            layer.add_module(f'fc{i}', nn.Linear(mlp_layers[i], mlp_layers[i+1], bias=True))\n",
    "            layer.add_module(f'act{i}', nn.Tanh())\n",
    "            self.model.add_module(f'layer{i}', layer)\n",
    "\n",
    "        last_layer = nn.Sequential()\n",
    "        last_layer.add_module(f'fc{len(mlp_layers)-2}', nn.Linear(mlp_layers[-2], mlp_layers[-1], bias=False))\n",
    "        self.model.add_module(f'layer{len(mlp_layers)-2}', last_layer)\n",
    "        \n",
    "#         for param in self.parameters():\n",
    "#             if len(param.shape) > 1:\n",
    "#                 nn.init.kaiming_normal_(param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f65a9ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (model): Sequential(\n",
      "    (first): Sequential(\n",
      "      (fc0): Linear(in_features=51, out_features=200, bias=True)\n",
      "      (act0): Tanh()\n",
      "    )\n",
      "    (block1): Sequential(\n",
      "      (fc1_0): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (act1_0): Tanh()\n",
      "      (fc1_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (act1_1): Tanh()\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (fc2_0): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (act2_0): Tanh()\n",
      "      (fc2_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      (act2_1): Tanh()\n",
      "    )\n",
      "    (last): Sequential(\n",
      "      (fc3): Linear(in_features=200, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "backbone = ResNet(args.backbone_layers)\n",
    "backbone = backbone.to(args.device)\n",
    "args.backbone = backbone\n",
    "print(args.backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc97d47b",
   "metadata": {},
   "source": [
    "## 数据集生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fac03e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import norm, exp, sin, cos, pi\n",
    "\n",
    "def norm2(x):\n",
    "    return norm(x, p=2, dim=1, keepdim=True)\n",
    "\n",
    "def u(x, t):\n",
    "    return exp(-t) * sin(pi/2 * (1 - norm2(x)).abs() ** 2.5)\n",
    "\n",
    "def g(x, t):\n",
    "    return torch.zeros_like(t)\n",
    "\n",
    "def h(x):\n",
    "    return sin(0.5 * pi * (1 - norm2(x)).abs() ** 2.5)\n",
    "\n",
    "def l(x):\n",
    "    return cos(0.5 * pi * (1 - norm2(x)).abs() ** 2.5)\n",
    "\n",
    "def k(x):\n",
    "    return -5/4 * pi * (x.shape[1] - 1) * norm2(x)**(-1) * l(x) * (1 - norm2(x)).abs() ** 1.5 \\\n",
    "        - 25/16 * pi**2 * h(x) * (1 - norm2(x)).abs() ** 3 \\\n",
    "        + 15/8 * pi * l(x) * (1 - norm2(x)).abs() ** 0.5\n",
    "\n",
    "def f(x, t):\n",
    "    return - exp(-t) * (h(x) + k(x)) - u(x, t) + u(x, t) ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9befe4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2400, 51]) torch.Size([1200, 51]) torch.Size([1200, 51]) torch.Size([2400, 1]) torch.Size([1200, 1]) torch.Size([1200, 1])\n"
     ]
    }
   ],
   "source": [
    "class Dataset_AllenCahnHD:\n",
    "    def __init__(self, bot_top, T, dim, device, N_max=100000):\n",
    "        self.bot, self.top = bot_top\n",
    "        self.T = T\n",
    "        self.dim = dim\n",
    "        self.device = device\n",
    "        self.N_max = N_max\n",
    "    \n",
    "    def train_data(self, N_r, N_b, N_0):\n",
    "        \"\"\"训练点采样\"\"\"\n",
    "        X_res = self.interior(N_r).to(self.device)\n",
    "        X_bcs = self.boundary(N_b).to(self.device)\n",
    "        X_ics = self.initial(N_0).to(self.device)\n",
    "        f_res = self.func_res(X_res).to(self.device)\n",
    "        u_bcs = self.func_bcs(X_bcs).to(self.device)\n",
    "        u_ics = self.func_ics(X_ics).to(self.device)\n",
    "        \n",
    "        return X_res, X_bcs, X_ics, f_res, u_bcs, u_ics\n",
    "    \n",
    "    def interior(self, N_r):\n",
    "        \"\"\"内部点采样\"\"\"\n",
    "        X_res = torch.Tensor(self.N_max, self.dim).normal_(0, 1)\n",
    "        X_res /= torch.norm(X_res, p=2, dim=1, keepdim=True)\n",
    "        X_res *= torch.Tensor(self.N_max, self.dim).uniform_(-1, 1)\n",
    "        \n",
    "        idx = torch.randperm(self.N_max)\n",
    "        idx = idx[:N_r]\n",
    "        \n",
    "        T = torch.Tensor(self.N_max, 1).uniform_(0, self.T)\n",
    "        X_res = torch.cat([X_res[idx], T[idx]], dim=1)  # 拼接X和T\n",
    "        return X_res\n",
    "    \n",
    "    def boundary(self, N_b):\n",
    "        \"\"\"边界点采样\"\"\"\n",
    "        X_bcs = torch.Tensor(self.N_max, self.dim).normal_(0, 1)\n",
    "        X_bcs /= torch.norm(X_bcs, p=2, dim=1, keepdim=True)\n",
    "\n",
    "        idx = torch.randperm(self.N_max)\n",
    "        idx = idx[:N_b]\n",
    "        \n",
    "        T = torch.Tensor(self.N_max, 1).uniform_(0, self.T)\n",
    "        X_bcs = torch.cat([X_bcs[idx], T[idx]], dim=1)  # 拼接X和T\n",
    "        return X_bcs\n",
    "    \n",
    "    def initial(self, N_0):\n",
    "        \"\"\"初始点采样\"\"\"\n",
    "        X_ics = torch.Tensor(self.N_max, self.dim).normal_(0, 1)\n",
    "        X_ics /= torch.norm(X_ics, p=2, dim=1, keepdim=True)\n",
    "        X_ics *= torch.Tensor(self.N_max, self.dim).uniform_(-1, 1)\n",
    "        idx = torch.randperm(self.N_max)\n",
    "        idx = idx[:N_0]\n",
    "        \n",
    "        T = torch.Tensor(self.N_max, 1).fill_(0.)\n",
    "        X_ics = torch.cat([X_ics[idx], T[idx]], dim=1)\n",
    "        return X_ics\n",
    "    \n",
    "    def func_res(self, X_res):\n",
    "        \"\"\"控制方程右端项\"\"\"\n",
    "        return f(x=X_res[:, :self.dim], t=X_res[:, [-1]])\n",
    "    \n",
    "    def func_bcs(self, X_bcs):\n",
    "        \"\"\"边界条件右端项\"\"\"\n",
    "        return g(x=X_bcs[:, :self.dim], t=X_bcs[:, [-1]])\n",
    "    \n",
    "    def func_ics(self, X_ics):\n",
    "        \"\"\"初始条件右端项\"\"\"\n",
    "        return h(x=X_ics[:, :self.dim])\n",
    "\n",
    "\n",
    "dataset = Dataset_AllenCahnHD(args.bot_top, args.T, args.dim, args.device)\n",
    "args.dataset = dataset\n",
    "X_res, X_bcs, X_ics, f_res, u_bcs, u_ics = dataset.train_data(args.N_r, args.N_b, args.N_0)\n",
    "print(X_res.shape, X_bcs.shape, X_ics.shape, f_res.shape, u_bcs.shape, u_ics.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35602c4b",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e662bb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "class Trainer_AllenCahnHD:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.dim = args.dim\n",
    "        self.device = args.device\n",
    "        \n",
    "        self.N_r = args.N_r\n",
    "        self.N_b = args.N_b\n",
    "        self.N_0 = args.N_0\n",
    "        self.dataset = args.dataset\n",
    "        \n",
    "        self.lam_res = args.lam_res\n",
    "        self.lam_bcs = args.lam_bcs\n",
    "        self.lam_ics = args.lam_ics\n",
    "        self.backbone = args.backbone\n",
    "        \n",
    "        self.model_name = self.backbone.__class__.__name__\n",
    "        self.model_path = self.get_model_path()\n",
    "        \n",
    "        self.iters_Adam = args.iters_Adam\n",
    "\n",
    "        # eps = np.finfo(np.float32).eps\n",
    "        self.optimizer_Adam = optim.Adam(self.backbone.parameters(), lr=args.backbone_lr, betas=(0.9, 0.999))\n",
    "        \n",
    "        self.step_size = args.step_size#int( self.iters_stage4 / (np.log(1e-3) / np.log(self.gamma)) )\n",
    "        self.scheduler = ExponentialLR(self.optimizer_Adam, gamma=args.backbone_gamma, verbose=True)\n",
    "        \n",
    "        # data\n",
    "        self.X_res, self.X_bcs, self.X_ics, self.f_res, self.u_bcs, self.u_ics = self.dataset.train_data(self.N_r, self.N_b, self.N_0)\n",
    "        \n",
    "        # Logger\n",
    "        self.logger = {\n",
    "            \"loss\": [],\n",
    "            \"loss_res\": [],\n",
    "            \"loss_bcs\": [],\n",
    "            \"loss_ics\": [],\n",
    "            \"iter\": []\n",
    "        }\n",
    "        \n",
    "        self.logger_valid = {\n",
    "            \"loss\": [],\n",
    "            \"loss_res\": [],\n",
    "            \"loss_bcs\": [],\n",
    "            \"loss_ics\": [],\n",
    "            \"iter\": [],\n",
    "            \"error\": []\n",
    "        }\n",
    "        \n",
    "    def get_model_path(self):\n",
    "        \"\"\"生成保存模型的路径\"\"\"\n",
    "        if not os.path.exists('models'):\n",
    "            os.mkdir('models')\n",
    "        \n",
    "        path = os.path.join('models', self.model_name)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        \n",
    "        return path\n",
    "        \n",
    "    def update_train_data(self):\n",
    "        \"\"\"更新采样点\"\"\"\n",
    "        self.X_res, self.X_bcs, self.X_ics, self.f_res, self.u_bcs, self.u_ics = self.dataset.train_data(self.N_r, self.N_b, self.N_0)\n",
    "\n",
    "    def net_u(self, X):\n",
    "        return self.backbone(X)\n",
    "\n",
    "    def net_r(self, xt):\n",
    "        xt.requires_grad_(True)\n",
    "        \n",
    "        u = self.net_u(xt)\n",
    "        grad_u = grad(u, xt)[0]\n",
    "        \n",
    "        u_t = grad_u[:, [-1]]\n",
    "        u_xx = torch.zeros(u.shape).to(xt.device)\n",
    "        for i in range(self.dim):\n",
    "            u_xx += grad(grad_u[:, [i]], xt)[0][:, [i]]\n",
    "        return u_t - u_xx - u + u**3\n",
    "    \n",
    "    def net_transformer(self, X, k):\n",
    "        X.requires_grad_(False)\n",
    "        return self.transformer(X, k)\n",
    "    \n",
    "    def compute_loss(self, use_transformer=False):\n",
    "        \"\"\"计算loss 可选择是否使用transformer\"\"\"\n",
    "        f_res_pred = self.net_r(self.X_res)\n",
    "        self.loss_res = torch.mean((f_res_pred - self.f_res) ** 2)\n",
    "\n",
    "        u_bcs_pred = self.net_u(self.X_bcs)\n",
    "        self.loss_bcs = torch.mean((u_bcs_pred - self.u_bcs) ** 2)\n",
    "\n",
    "        u_ics_pred = self.net_u(self.X_ics)\n",
    "        self.loss_ics = torch.mean((u_ics_pred - self.u_ics) ** 2)\n",
    "\n",
    "        self.loss = self.lam_res * self.loss_res + self.lam_bcs * self.loss_bcs + self.lam_ics * self.loss_ics\n",
    "\n",
    "    def log_loss(self):\n",
    "        \"\"\"记录当前loss至logger字典\"\"\"\n",
    "        self.logger[\"loss\"].append(self.loss.item())\n",
    "        self.logger[\"loss_res\"].append(self.loss_res.item())\n",
    "        self.logger[\"loss_bcs\"].append(self.loss_bcs.item())\n",
    "        self.logger[\"loss_ics\"].append(self.loss_ics.item())\n",
    "        self.logger[\"iter\"].append(self.iter + 1)\n",
    "\n",
    "    def log_info(self):\n",
    "        \"\"\"保存并打印训练信息\"\"\"\n",
    "        info = f'Iter {self.iter+1:5d} Time:{time.time()-self.start_time:.1e} # ' + \\\n",
    "               f'Loss:{self.loss.item():.2e}, Loss_r:{self.loss_res.item():.2e}, Loss_b:{self.loss_bcs.item():.2e}, Loss_0:{self.loss_ics.item():.2e} # ' + \\\n",
    "               f'Valid:{self.valid_loss_value:.2e}, RL2:{self.error_u:.2e}'\n",
    "        with open(\"train_info.txt\", 'a') as f:\n",
    "            f.write(info + '\\n')\n",
    "        print(info)\n",
    "        \n",
    "    def compute_rl2(self, X_res):\n",
    "        \"\"\"计算relative l2 error\"\"\"\n",
    "        u_star = u(x=X_res[:, :-1], t=X_res[:, [-1]])  # 计算解析解\n",
    "        u_star = u_star.detach().cpu().numpy()\n",
    "        \n",
    "        u_pred = self.net_u(X_res)  # 计算预测解\n",
    "        u_pred = u_pred.detach().cpu().numpy()\n",
    "        \n",
    "        error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)  # rl2 error\n",
    "        return error_u\n",
    "\n",
    "    def valid(self):\n",
    "        \"\"\"验证并保存最优模型\"\"\"\n",
    "        # 计算loss\n",
    "        X_res, X_bcs, X_ics, f_res, u_bcs, u_ics = self.dataset.train_data(1000, 1000, 1000)\n",
    "        self.backbone.eval()\n",
    "        # 不经过transformer的loss\n",
    "        f_res_pred = self.net_r(X_res)\n",
    "        u_bcs_pred = self.net_u(X_bcs)\n",
    "        u_ics_pred = self.net_u(X_ics)\n",
    "        loss_res = torch.mean((f_res_pred - f_res) ** 2)\n",
    "        loss_bcs = torch.mean((u_bcs_pred - u_bcs) ** 2)\n",
    "        loss_ics = torch.mean((u_ics_pred - u_ics) ** 2)\n",
    "        loss = self.lam_res * loss_res + self.lam_bcs * loss_bcs + self.lam_ics * loss_ics\n",
    "        \n",
    "        # 计算relative l2 error\n",
    "        self.error_u = self.compute_rl2(X_res)\n",
    "        \n",
    "        self.backbone.train()\n",
    "        \n",
    "        # 记录valid的loss和rl2 error信息\n",
    "        self.logger_valid[\"loss\"].append(loss.item())\n",
    "        self.logger_valid[\"loss_res\"].append(loss_res.item())\n",
    "        self.logger_valid[\"loss_bcs\"].append(loss_bcs.item())\n",
    "        self.logger_valid[\"loss_ics\"].append(loss_ics.item())\n",
    "        self.logger_valid[\"iter\"].append(self.iter + 1)\n",
    "        self.logger_valid[\"error\"].append(self.error_u)\n",
    "        \n",
    "        # 验证模型loss是否最优并选择保存\n",
    "        self.valid_loss_value = loss.item()\n",
    "        is_best = self.valid_loss_value < self.best_loss\n",
    "        if is_best:\n",
    "            self.best_loss = self.valid_loss_value\n",
    "        state = {\n",
    "            'iter': self.iter,\n",
    "            'state_dict': self.backbone.state_dict(),\n",
    "            'best_loss': self.best_loss\n",
    "        }\n",
    "        save_model(state, is_best, save_dir=self.model_path)\n",
    "    \n",
    "    def train(self):\n",
    "        self.start_time = time.time()\n",
    "        self.best_loss = 1.e10\n",
    "        \n",
    "        self.iter = 0\n",
    "        for _ in range(self.iters_Adam):\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            self.compute_loss()\n",
    "            self.loss.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "            self.log_loss()\n",
    "\n",
    "            if (self.iter + 1) % 100 == 0:\n",
    "                self.valid()\n",
    "                self.log_info()\n",
    "                self.update_train_data()\n",
    "\n",
    "            if (self.iter + 1) % self.step_size == 0:\n",
    "                self.scheduler.step()\n",
    "            self.iter += 1\n",
    "            \n",
    "            \n",
    "trainer = Trainer_AllenCahnHD(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6972fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   100 Time:1.1e+01 # Loss:6.95e+02, Loss_r:4.75e+02, Loss_b:1.36e+00, Loss_0:8.40e-01 # Valid:2.08e+03, RL2:5.75e+00\n",
      "Iter   200 Time:2.2e+01 # Loss:5.42e+02, Loss_r:4.20e+02, Loss_b:7.39e-01, Loss_0:4.81e-01 # Valid:1.51e+03, RL2:4.68e+00\n",
      "Iter   300 Time:3.3e+01 # Loss:4.56e+02, Loss_r:3.80e+02, Loss_b:4.32e-01, Loss_0:3.26e-01 # Valid:2.89e+03, RL2:3.62e+00\n",
      "Iter   400 Time:4.4e+01 # Loss:3.79e+02, Loss_r:3.44e+02, Loss_b:2.50e-01, Loss_0:9.90e-02 # Valid:4.38e+02, RL2:2.39e+00\n",
      "Iter   500 Time:5.5e+01 # Loss:3.49e+02, Loss_r:3.29e+02, Loss_b:1.39e-01, Loss_0:6.02e-02 # Valid:3.82e+02, RL2:2.15e+00\n",
      "Iter   600 Time:6.6e+01 # Loss:3.20e+02, Loss_r:3.07e+02, Loss_b:8.92e-02, Loss_0:4.39e-02 # Valid:3.81e+02, RL2:1.78e+00\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4883df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./loss_logger_dict.npy\", trainer.logger)\n",
    "np.save(\"./loss_logger_valid_dict.npy\", trainer.logger_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70328ecd",
   "metadata": {},
   "source": [
    "## 结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7434af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':18})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32348622",
   "metadata": {},
   "source": [
    "### 训练的loss图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss图\n",
    "loss_logger = np.load(\"./loss_logger_dict.npy\", allow_pickle=True).item()\n",
    "\n",
    "fig = plt.figure(figsize=(9, 7), dpi=64)\n",
    "ax = fig.subplots()\n",
    "\n",
    "_k = 100\n",
    "# ax.plot(loss_logger[\"iter\"][::k], loss_logger[\"loss\"][::k], label=r\"$\\mathcal{L}$\", linewidth=3)\n",
    "ax.plot(loss_logger[\"iter\"][::_k], loss_logger[\"loss_res\"][::_k], label=r\"$\\mathcal{L}_{r}$\", linewidth=3)\n",
    "ax.plot(loss_logger[\"iter\"][::_k], loss_logger[\"loss_bcs\"][::_k], label=r\"$\\mathcal{L}_{b}$\", linewidth=3)\n",
    "ax.plot(loss_logger[\"iter\"][::_k], loss_logger[\"loss_ics\"][::_k], label=r\"$\\mathcal{L}_{0}$\", linewidth=3)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.grid()\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Loss.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac13fa6b",
   "metadata": {},
   "source": [
    "### 验证的loss图 (包括只经backbone和经过transformer和backbone的loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_logger_valid = np.load(\"./loss_logger_valid_dict.npy\", allow_pickle=True).item()\n",
    "\n",
    "fig = plt.figure(figsize=(9, 7), dpi=64)\n",
    "ax = fig.subplots()\n",
    "\n",
    "_k = 1\n",
    "# ax.plot(loss_logger_valid[\"iter\"][::k], loss_logger_valid[\"loss\"][::k], label=r\"$\\mathcal{L}$\", linewidth=3)\n",
    "ax.plot(loss_logger_valid[\"iter\"][::_k], loss_logger_valid[\"loss_res\"][::_k], label=r\"$\\mathcal{L}_{r}$\", linewidth=3)\n",
    "ax.plot(loss_logger_valid[\"iter\"][::_k], loss_logger_valid[\"loss_bcs\"][::_k], label=r\"$\\mathcal{L}_{b}$\", linewidth=3)\n",
    "ax.plot(loss_logger_valid[\"iter\"][::_k], loss_logger_valid[\"loss_ics\"][::_k], label=r\"$\\mathcal{L}_{0}$\", linewidth=3)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.grid()\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('loss_valid.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649be43",
   "metadata": {},
   "source": [
    "### 验证的Relative L2 error图 (包括只经backbone和经过transformer和backbone的rl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07114da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid中记录的relative l2 error记录 第一行只经过backbone 第二行经过transformer和backbone\n",
    "loss_logger_valid = np.load(\"./loss_logger_valid_dict.npy\", allow_pickle=True).item()\n",
    "\n",
    "fig = plt.figure(figsize=(9, 7), dpi=64)\n",
    "ax = fig.subplots()\n",
    "\n",
    "_k = 1\n",
    "ax.plot(loss_logger_valid[\"iter\"][::_k], loss_logger_valid[\"error\"][::_k], label=r\"PINN\", color='k', linewidth=3)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Relative error\")\n",
    "ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Relative_L2_error.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b8aaf7",
   "metadata": {},
   "source": [
    "### Relative L2 error (仅考虑内部点）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用最好的模型\n",
    "backbone = ResNet(args.backbone_layers)\n",
    "state_dict = torch.load(f'{trainer.model_path}/best_model.pth')\n",
    "backbone.load_state_dict(state_dict['state_dict'])\n",
    "backbone.eval()\n",
    "print(\"Iter:\\t\", state_dict['iter'] + 1)\n",
    "print(\"Loss:\\t\", \"{:.2e}\".format(state_dict['best_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_sol(xt):\n",
    "    # 解析解\n",
    "    return u(x=xt[:, :-1], t=xt[:, [-1]])\n",
    "\n",
    "# 采样内部点\n",
    "dataset = Dataset_AllenCahnHD(args.bot_top, args.T, args.dim, torch.device('cpu'))\n",
    "X_res, _, _, _, _, _ = dataset.train_data(N_r=10000, N_b=0, N_0=0)\n",
    "# 计算u_star u_pred\n",
    "u_star = exact_sol(X_res)\n",
    "u_pred = backbone(X_res)\n",
    "\n",
    "u_star = u_star.detach().numpy()\n",
    "u_pred = u_pred.detach().numpy()\n",
    "#计算L2相对误差\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "with open(\"RL2.txt\", 'w') as f_:\n",
    "    f_.write(str(error_u))\n",
    "print('Relative L2 error: {:.3e}'.format(error_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319dfb88",
   "metadata": {},
   "source": [
    "### 可视化内部(x1, x2, 0, ..., 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 101\n",
    "dim = args.dim\n",
    "# 取前两维\n",
    "x1 = np.linspace(-1, 1, n)\n",
    "x2 = np.linspace(-1, 1, n)\n",
    "x1, x2 = np.meshgrid(x1, x2)\n",
    "# x3, ..., x50 = 0\n",
    "x12 = np.zeros((n*n, dim))\n",
    "x12[:, [0]] = x1.reshape(-1, 1)\n",
    "x12[:, [1]] = x2.reshape(-1, 1)\n",
    "t = np.ones((x12.shape[0], 1)) * args.T\n",
    "x12 = np.concatenate([x12, t], axis=1)\n",
    "x12 = torch.from_numpy(x12).float()\n",
    "# 解析解和预测解\n",
    "u12_star = exact_sol(x12)\n",
    "u12_pred = backbone(x12)\n",
    "\n",
    "u12_star = u12_star.detach().numpy()\n",
    "u12_pred = u12_pred.detach().numpy()\n",
    "\n",
    "u12_star = u12_star.reshape(x1.shape)\n",
    "u12_pred = u12_pred.reshape(x1.shape)\n",
    "\n",
    "for i in range(u12_star.shape[0]):\n",
    "    for j in range(u12_star.shape[1]):\n",
    "        if x1[i, j]**2 + x2[i, j]**2 > 1.:\n",
    "            u12_star[i, j] = np.nan\n",
    "            u12_pred[i, j] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f293297",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "axes = fig.subplots(1, 3)\n",
    "\n",
    "cax1 = axes[0].pcolor(x1, x2, u12_star, cmap='jet')\n",
    "fig.colorbar(cax1)\n",
    "axes[0].set_xlabel('$x_1$')\n",
    "axes[0].set_ylabel('$x_2$')\n",
    "axes[0].set_title(r'Reference $u$')\n",
    "axes[0].set_xlim([-1, 1])\n",
    "axes[0].set_ylim([-1, 1])\n",
    "axes[0].set_xticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[0].set_yticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[0].set_aspect(1./axes[0].get_data_ratio())\n",
    "\n",
    "cax2 = axes[1].pcolor(x1, x2, u12_pred, cmap='jet')\n",
    "fig.colorbar(cax2)\n",
    "axes[1].set_xlabel('$x_1$')\n",
    "axes[1].set_ylabel('$x_2$')\n",
    "axes[1].set_title(r'Predicted $u$')\n",
    "axes[1].set_xlim([-1, 1])\n",
    "axes[1].set_ylim([-1, 1])\n",
    "axes[1].set_xticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[1].set_yticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[1].set_aspect(1./axes[1].get_data_ratio())\n",
    "\n",
    "cax3 = axes[2].pcolor(x1, x2, np.abs(u12_star - u12_pred), cmap='jet')\n",
    "fig.colorbar(cax3)\n",
    "axes[2].set_xlabel('$x_1$')\n",
    "axes[2].set_ylabel('$x_2$')\n",
    "axes[2].set_title('Absolute error')\n",
    "axes[2].set_xlim([-1, 1])\n",
    "axes[2].set_ylim([-1, 1])\n",
    "axes[2].set_xticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[2].set_yticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[2].set_aspect(1./axes[2].get_data_ratio())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'Allen_Cahn_{args.dim}d_result.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e2a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6.5, 5), dpi=100)\n",
    "plot = ax.contourf(x1, x2, np.abs(u12_star - u12_pred), levels=16, cmap='jet')\n",
    "\n",
    "cbar = fig.colorbar(plot)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "cbar.update_ticks()\n",
    "\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_xticks([-1, -0.5, 0, 0.5, 1])\n",
    "ax.set_yticks([-1, -0.5, 0, 0.5, 1])\n",
    "ax.set_title('Absolute error')\n",
    "ax.set_aspect(1./ax.get_data_ratio())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./Absolute_error', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc5562b",
   "metadata": {},
   "source": [
    "## Transformer变换可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f2864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loss_res\n",
    "# trainer.device = torch.device('cpu')\n",
    "# trainer.backbone = trainer.backbone.cpu()\n",
    "\n",
    "# f_res = trainer.dataset.func_res(x12)\n",
    "# f_res_pred = trainer.net_r(x12)\n",
    "# loss_res = (f_res - f_res_pred) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(6.5, 5))\n",
    "# ax = fig.subplots()\n",
    "\n",
    "# cax = ax.scatter(x12.detach().numpy()[:, 0], x12.detach().numpy()[:, 1], \n",
    "#                  c=loss_res.detach().cpu().numpy(), \n",
    "# #                  vmin=0, vmax=5e-5,\n",
    "#                  cmap=\"jet\", s=80)\n",
    "# fig.colorbar(cax)\n",
    "# ax.set_xlabel('$x_1$')\n",
    "# ax.set_ylabel('$x_2$')\n",
    "# ax.set_title(r'Residual')\n",
    "# ax.set_xlim([-1, 1])\n",
    "# ax.set_ylim([-1, 1])\n",
    "# ax.set_xticks(np.arange(-1, 1.1, 0.5))\n",
    "# ax.set_yticks(np.arange(-1, 1.1, 0.5))\n",
    "# ax.set_aspect(1./axes[0].get_data_ratio())\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b663924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 重新取点\n",
    "# n = 21\n",
    "# dim = args.dim\n",
    "# # 取前两维\n",
    "# xx1 = np.linspace(-1, 1, n)\n",
    "# xx2 = np.linspace(-1, 1, n)\n",
    "# xx1, xx2 = np.meshgrid(xx1, xx2)\n",
    "# # x3, ..., x50 = 0\n",
    "# xx12 = np.zeros((n*n, dim))\n",
    "# xx12[:, [0]] = xx1.reshape(-1, 1)\n",
    "# xx12[:, [1]] = xx2.reshape(-1, 1)\n",
    "# t = np.ones((xx12.shape[0], 1)) * args.T\n",
    "# xx12 = np.concatenate([xx12, t], axis=1)\n",
    "# xx12 = torch.from_numpy(xx12).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd37916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 变换后的点\n",
    "# # model = MLP(args.subnet_layers)  # 保存的loss最低时的transformer\n",
    "# # transformer = TransformerNet(model, args.bot_top)\n",
    "\n",
    "# # state_dict = torch.load(f'{trainer.model_path}/best_model.pth')\n",
    "# # transformer.load_state_dict(state_dict['state_dict_transformer'])\n",
    "# # transformer.eval()\n",
    "\n",
    "# # xx12_ = transformer(xx12)\n",
    "\n",
    "# trainer.transformer = trainer.transformer.cpu()  # 训练最后的transformer\n",
    "# f_xx12 = dataset.func_res(xx12)\n",
    "# k = trainer.update_k(xx12, f_xx12)\n",
    "# xx12_ = trainer.net_transformer(xx12, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(k.detach().cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3535643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(12, 5))\n",
    "# axes = fig.subplots(1, 2)\n",
    "\n",
    "# axes[0].scatter(xx12.detach().numpy()[:, 0], xx12.detach().numpy()[:, 1], s=30, color='b')\n",
    "# axes[0].set_xlabel('$x_1$')\n",
    "# axes[0].set_ylabel('$x_2$')\n",
    "# axes[0].set_title(r'Original distribution')\n",
    "# axes[0].set_xlim([-1, 1])\n",
    "# axes[0].set_ylim([-1, 1])\n",
    "# axes[0].set_xticks(np.arange(-1, 1.1, 0.5))\n",
    "# axes[0].set_yticks(np.arange(-1, 1.1, 0.5))\n",
    "# axes[0].set_aspect(1./axes[0].get_data_ratio())\n",
    "\n",
    "# axes[1].scatter(xx12_.detach().numpy()[:, 0], xx12_.detach().numpy()[:, 1], s=30, color='r')\n",
    "# axes[1].set_xlabel('$x_1$')\n",
    "# axes[1].set_ylabel('$x_2$')\n",
    "# axes[1].set_title('Transformed distribution')\n",
    "# axes[1].set_xlim([-1, 1])\n",
    "# axes[1].set_ylim([-1, 1])\n",
    "# axes[1].set_xticks(np.arange(-1, 1.1, 0.5))\n",
    "# axes[1].set_yticks(np.arange(-1, 1.1, 0.5))\n",
    "# axes[1].set_aspect(1./axes[1].get_data_ratio())\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6fc96a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch39",
   "language": "python",
   "name": "torch39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
