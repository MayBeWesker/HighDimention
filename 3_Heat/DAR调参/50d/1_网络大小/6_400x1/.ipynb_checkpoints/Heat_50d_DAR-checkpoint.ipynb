{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e12bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_179844/82325636.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a934c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f13bf767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "class Options_HeatHD:\n",
    "    def __init__(self):\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--cuda', default=True, help='if you use cuda')\n",
    "        \n",
    "        parser.add_argument('--bot_top', type=tuple, default=(-1, 1), help='a tuple of the form (bot, top)')\n",
    "        parser.add_argument('--T', type=float, default=1., help='a float T of time domain [0, T]')\n",
    "        parser.add_argument('--dim', default=50, help='dimension')\n",
    "        parser.add_argument('--N_r', default=2400, help='num of interior points')\n",
    "        parser.add_argument('--N_b', default=1200, help='num of boundary points')\n",
    "        parser.add_argument('--N_0', default=1200, help='num of initial points')\n",
    "        \n",
    "        parser.add_argument('--backbone_layers', type=list, default=([60]*5), help='list of nn layers of backbone')\n",
    "        parser.add_argument('--subnet_layers', type=list, default=([400]*1), help='list of nn layers of subnet')\n",
    "        \n",
    "        parser.add_argument('--backbone_lr', type=float, default=1e-3, help='initial learning rate of backbone')\n",
    "        parser.add_argument('--subnet_lr', type=float, default=5e-3, help='initial learning rate of subnet')\n",
    "        \n",
    "        parser.add_argument('--backbone_gamma', type=float, default=0.95, help='gamma in lr_scheduler for backbone optimizer')\n",
    "        parser.add_argument('--subnet_gamma', type=float, default=0.95, help='gamma in lr_scheduler for subnet optimizer')\n",
    "        parser.add_argument('--step_size', type=int, default=1200, help='step_size of lr_scheduler for Adam optimizer')\n",
    "        \n",
    "        parser.add_argument('--iters_stage1', type=int, default=0, help='iters for stage 1 used Adam')\n",
    "        parser.add_argument('--iters_stage2', type=int, default=0, help='iters for stage 2 used Adam')\n",
    "        parser.add_argument('--iters_stage3', type=int, default=24000, help='iters for stage 3 used Adam')\n",
    "        parser.add_argument('--iters_stage4', type=int, default=0, help='iters for stage 4 used Adam')\n",
    "        \n",
    "        parser.add_argument('--lam_res', type=float, default=1, help='weight of loss_res')\n",
    "        parser.add_argument('--lam_bcs', type=float, default=1, help='weight of loss_bcs')\n",
    "        parser.add_argument('--lam_ics', type=float, default=1, help='weight of loss_ics')\n",
    "        parser.add_argument('--K1', type=int, default=500, help='for stage3, optimize the backbone K1 times every (K1+K2) iters')\n",
    "        parser.add_argument('--K2', type=int, default=100, help='for stage3, optimize the transformer K2 times every (K1+K2) iters')\n",
    "        parser.add_argument('--k_scale', type=float, default=0.1, help='the scale of transformer limited residue')\n",
    "        \n",
    "        self.parser = parser\n",
    "\n",
    "    def parse_default(self):\n",
    "        args = self.parser.parse_args(args=[])\n",
    "        args.device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "        \n",
    "        # 增加网络输入输出层\n",
    "        args.backbone_layers = [args.dim + 1] + args.backbone_layers + [1]\n",
    "        args.subnet_layers = [args.dim + 1] + args.subnet_layers + [args.dim + 1]\n",
    "        return args\n",
    "    \n",
    "\n",
    "args = Options_HeatHD().parse_default()\n",
    "print(args.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4f2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(state, is_best=None, save_dir=None):\n",
    "    last_model = os.path.join(save_dir, 'last_model.pth')\n",
    "    torch.save(state, last_model)\n",
    "    if is_best:\n",
    "        best_model = os.path.join(save_dir, 'best_model.pth')\n",
    "        shutil.copyfile(last_model, best_model)\n",
    "\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "\n",
    "def grad(outputs, inputs):\n",
    "    \"\"\" compute the derivative of outputs associated with inputs\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "    outputs: (N, 1) tensor\n",
    "    inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs,\n",
    "                               grad_outputs=torch.ones_like(outputs),\n",
    "                               create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6e3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06429a",
   "metadata": {},
   "source": [
    "## 网络模型 (Modified ResNet / ResNet / Modified MLP / MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb86f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedResNet(nn.Module):\n",
    "    def __init__(self, mlp_layers):\n",
    "        super(ModifiedResNet, self).__init__()\n",
    "        \n",
    "        self.encoder_u = nn.Sequential()\n",
    "        self.encoder_u.add_module('fc_u', nn.Linear(mlp_layers[0], mlp_layers[1], bias=True))\n",
    "        self.encoder_u.add_module('act_u', nn.Tanh())\n",
    "        \n",
    "        self.encoder_v = nn.Sequential()\n",
    "        self.encoder_v.add_module('fc_v', nn.Linear(mlp_layers[0], mlp_layers[1], bias=True))\n",
    "        self.encoder_v.add_module('act_v', nn.Tanh())\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        \n",
    "        first_layer = nn.Sequential()\n",
    "        first_layer.add_module(f'fc0', nn.Linear(mlp_layers[0], mlp_layers[1], bias=True))\n",
    "        first_layer.add_module(f'act0', nn.Tanh())\n",
    "        self.model.add_module(f'first', first_layer)\n",
    "        \n",
    "        for i in range(1, len(mlp_layers)-2):\n",
    "            block = nn.Sequential()\n",
    "            block.add_module(f'fc{i}_0', nn.Linear(mlp_layers[i], mlp_layers[i+1], bias=True))\n",
    "            block.add_module(f'act{i}_0', nn.Tanh())\n",
    "            block.add_module(f'fc{i}_1', nn.Linear(mlp_layers[i], mlp_layers[i+1], bias=True))\n",
    "            block.add_module(f'act{i}_1', nn.Tanh())\n",
    "            self.model.add_module(f'block{i}', block)\n",
    "\n",
    "        last_layer = nn.Sequential()\n",
    "        last_layer.add_module(f'fc{len(mlp_layers)-2}', nn.Linear(mlp_layers[-2], mlp_layers[-1], bias=False))\n",
    "        self.model.add_module(f'last', last_layer)\n",
    "        \n",
    "#         for param in self.parameters():\n",
    "#             if len(param.shape) > 1:\n",
    "#                 nn.init.kaiming_normal_(param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        u = self.encoder_u(X)\n",
    "        v = self.encoder_v(X)\n",
    "        \n",
    "        X = self.model[0](X)\n",
    "        for i_block in range(1, len(self.model) - 1):\n",
    "            X_ = self.model[i_block](X)\n",
    "            X = X + X_\n",
    "            X = X / 2.\n",
    "            X = (1 - X) * u + X * v\n",
    "        return self.model[-1](X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3fcf7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, mlp_layers):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        \n",
    "        first_layer = nn.Sequential()\n",
    "        first_layer.add_module(f'fc0', nn.Linear(mlp_layers[0], mlp_layers[1], bias=True))\n",
    "        first_layer.add_module(f'act0', nn.Tanh())\n",
    "        self.model.add_module(f'first', first_layer)\n",
    "        \n",
    "        for i in range(1, len(mlp_layers)-2):\n",
    "            block = nn.Sequential()\n",
    "            block.add_module(f'fc{i}_0', nn.Linear(mlp_layers[i], mlp_layers[i+1], bias=True))\n",
    "            block.add_module(f'act{i}_0', nn.Tanh())\n",
    "            block.add_module(f'fc{i}_1', nn.Linear(mlp_layers[i], mlp_layers[i+1], bias=True))\n",
    "            block.add_module(f'act{i}_1', nn.Tanh())\n",
    "            self.model.add_module(f'block{i}', block)\n",
    "\n",
    "        last_layer = nn.Sequential()\n",
    "        last_layer.add_module(f'fc{len(mlp_layers)-2}', nn.Linear(mlp_layers[-2], mlp_layers[-1], bias=False))\n",
    "        self.model.add_module(f'last', last_layer)\n",
    "        \n",
    "#         for param in self.parameters():\n",
    "#             if len(param.shape) > 1:\n",
    "#                 nn.init.kaiming_normal_(param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.model[0](X)\n",
    "        for i_block in range(1, len(self.model) - 1):\n",
    "            X_ = self.model[i_block](X)\n",
    "            X = X_ + X\n",
    "        return self.model[-1](X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5393d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedMLP(nn.Module):\n",
    "    def __init__(self, mlp_layers):\n",
    "        super(ModifiedMLP, self).__init__()\n",
    "        \n",
    "        self.encoder_u = nn.Sequential()\n",
    "        self.encoder_u.add_module('fc_u', nn.Linear(mlp_layers[0], mlp_layers[1], bias=True))\n",
    "        self.encoder_u.add_module('act_u', nn.Tanh())\n",
    "        \n",
    "        self.encoder_v = nn.Sequential()\n",
    "        self.encoder_v.add_module('fc_v', nn.Linear(mlp_layers[0], mlp_layers[1], bias=True))\n",
    "        self.encoder_v.add_module('act_v', nn.Tanh())\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        for i in range(len(mlp_layers)-2):\n",
    "            layer = nn.Sequential()\n",
    "            layer.add_module(f'fc{i}', nn.Linear(mlp_layers[i], mlp_layers[i+1], bias=True))\n",
    "            layer.add_module(f'act{i}', nn.Tanh())\n",
    "            self.model.add_module(f'layer{i}', layer)\n",
    "\n",
    "        last_layer = nn.Sequential()\n",
    "        last_layer.add_module(f'fc{len(mlp_layers)-2}', nn.Linear(mlp_layers[-2], mlp_layers[-1], bias=False))\n",
    "        self.model.add_module(f'layer{len(mlp_layers)-2}', last_layer)\n",
    "        \n",
    "#         for param in self.parameters():\n",
    "#             if len(param.shape) > 1:\n",
    "#                 nn.init.kaiming_normal_(param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        u = self.encoder_u(X)\n",
    "        v = self.encoder_v(X)\n",
    "        \n",
    "        for i in range(len(self.model) - 1):\n",
    "            X = self.model[i](X)\n",
    "            X = X / 2.\n",
    "            X = (1 - X) * u + X * v\n",
    "        return self.model[-1](X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22ca867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, mlp_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential()\n",
    "        for i in range(len(mlp_layers)-2):\n",
    "            layer = nn.Sequential()\n",
    "            layer.add_module(f'fc{i}', nn.Linear(mlp_layers[i], mlp_layers[i+1], bias=True))\n",
    "            layer.add_module(f'act{i}', nn.Tanh())\n",
    "            self.model.add_module(f'layer{i}', layer)\n",
    "\n",
    "        last_layer = nn.Sequential()\n",
    "        last_layer.add_module(f'fc{len(mlp_layers)-2}', nn.Linear(mlp_layers[-2], mlp_layers[-1], bias=False))\n",
    "        self.model.add_module(f'layer{len(mlp_layers)-2}', last_layer)\n",
    "        \n",
    "#         for param in self.parameters():\n",
    "#             if len(param.shape) > 1:\n",
    "#                 nn.init.kaiming_normal_(param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f65a9ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (model): Sequential(\n",
      "    (first): Sequential(\n",
      "      (fc0): Linear(in_features=51, out_features=60, bias=True)\n",
      "      (act0): Tanh()\n",
      "    )\n",
      "    (block1): Sequential(\n",
      "      (fc1_0): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (act1_0): Tanh()\n",
      "      (fc1_1): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (act1_1): Tanh()\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (fc2_0): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (act2_0): Tanh()\n",
      "      (fc2_1): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (act2_1): Tanh()\n",
      "    )\n",
      "    (block3): Sequential(\n",
      "      (fc3_0): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (act3_0): Tanh()\n",
      "      (fc3_1): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (act3_1): Tanh()\n",
      "    )\n",
      "    (block4): Sequential(\n",
      "      (fc4_0): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (act4_0): Tanh()\n",
      "      (fc4_1): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (act4_1): Tanh()\n",
      "    )\n",
      "    (last): Sequential(\n",
      "      (fc5): Linear(in_features=60, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "backbone = ResNet(args.backbone_layers)\n",
    "backbone = backbone.to(args.device)\n",
    "args.backbone = backbone\n",
    "print(args.backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9401b526",
   "metadata": {},
   "source": [
    "## 对抗TransformerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01b39510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerNet(\n",
      "  (model): MLP(\n",
      "    (model): Sequential(\n",
      "      (layer0): Sequential(\n",
      "        (fc0): Linear(in_features=51, out_features=400, bias=True)\n",
      "        (act0): Tanh()\n",
      "      )\n",
      "      (layer1): Sequential(\n",
      "        (fc1): Linear(in_features=400, out_features=51, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self, model, bot_top):\n",
    "        super(TransformerNet, self).__init__()\n",
    "        self.model = model\n",
    "        bot, top = bot_top\n",
    "        self.length = top - bot\n",
    "        self.mid = (bot + top) / 2\n",
    "        \n",
    "    def input_transform(self, X):\n",
    "        \"\"\"输入变换 压缩到[-1,1]再传入arcsin\"\"\"\n",
    "        return torch.arcsin((X - self.mid) / self.length * 2.) / torch.pi\n",
    "    \n",
    "    def output_transform(self, X):\n",
    "        # to PDE's domain 非线性放缩\n",
    "        return torch.sin(torch.pi * X) / 2. * self.length + self.mid\n",
    "    \n",
    "    def forward(self, X, residual_limit):\n",
    "        X = self.input_transform(X)\n",
    "        X_ = self.model(X)\n",
    "        \n",
    "        X_ = torch.tanh(X_) * residual_limit  # 限制变换残差的界\n",
    "        \n",
    "        X = self.output_transform(X + X_)\n",
    "        return X\n",
    "    \n",
    "\n",
    "model = MLP(args.subnet_layers)\n",
    "transformer = TransformerNet(model, args.bot_top)\n",
    "transformer = transformer.to(args.device)\n",
    "args.transformer = transformer\n",
    "print(args.transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc97d47b",
   "metadata": {},
   "source": [
    "## 数据集生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9befe4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2400, 51]) torch.Size([1200, 51]) torch.Size([1200, 51]) torch.Size([2400, 1]) torch.Size([1200, 1]) torch.Size([1200, 1])\n"
     ]
    }
   ],
   "source": [
    "class Dataset_HeatHD:\n",
    "    def __init__(self, bot_top, T, dim, device, N_max=100000):\n",
    "        self.bot, self.top = bot_top\n",
    "        self.T = T\n",
    "        self.dim = dim\n",
    "        self.device = device\n",
    "        self.N_max = N_max\n",
    "    \n",
    "    def train_data(self, N_r, N_b, N_0):\n",
    "        \"\"\"训练点采样\"\"\"\n",
    "        X_res = self.interior(N_r).to(self.device)\n",
    "        X_bcs = self.boundary(N_b).to(self.device)\n",
    "        X_ics = self.initial(N_0).to(self.device)\n",
    "        f_res = self.func_res(X_res).to(self.device)\n",
    "        u_bcs = self.func_bcs(X_bcs).to(self.device)\n",
    "        u_ics = self.func_ics(X_ics).to(self.device)\n",
    "        \n",
    "        return X_res, X_bcs, X_ics, f_res, u_bcs, u_ics\n",
    "    \n",
    "    def interior(self, N_r):\n",
    "        \"\"\"内部点采样\"\"\"\n",
    "        X_res = torch.Tensor(self.N_max, self.dim).uniform_(self.bot, self.top)\n",
    "        idx = torch.randperm(self.N_max)\n",
    "        idx = idx[:N_r]\n",
    "        \n",
    "        T = torch.Tensor(self.N_max, 1).uniform_(0, self.T)\n",
    "        X_res = torch.cat([X_res[idx], T[idx]], dim=1)  # 拼接X和T\n",
    "        return X_res\n",
    "    \n",
    "    def boundary(self, N_b):\n",
    "        \"\"\"边界点采样\"\"\"\n",
    "        X_bcs = torch.Tensor(self.N_max, self.dim).uniform_(self.bot, self.top)\n",
    "        bots = self.bot * torch.ones(1, 1)\n",
    "        tops = self.top * torch.ones(1, 1)\n",
    "        \n",
    "        n = int(self.N_max / self.dim / 2)\n",
    "        num = [n * i for i in range(2 * self.dim)]\n",
    "        num[0] = 0\n",
    "        num.append(self.N_max)\n",
    "        \n",
    "        for i in range(self.dim):  # 逐个维度设为下界bot 以及上界top 得到边界点\n",
    "            X_bcs[num[2 * i]:num[2 * i + 1], i] = bots.repeat(num[2 * i + 1] - num[2 * i], 1).squeeze()\n",
    "            X_bcs[num[2 * i + 1]:num[2 * i + 2], i] = tops.repeat(num[2 * i + 2] - num[2 * i + 1], 1).squeeze()\n",
    "\n",
    "        idx = torch.randperm(self.N_max)\n",
    "        idx = idx[:N_b]\n",
    "        \n",
    "        T = torch.Tensor(self.N_max, 1).uniform_(0, self.T)\n",
    "        X_bcs = torch.cat([X_bcs[idx], T[idx]], dim=1)  # 拼接X和T\n",
    "        return X_bcs\n",
    "    \n",
    "    def initial(self, N_0):\n",
    "        \"\"\"初始点采样\"\"\"\n",
    "        X_ics = torch.Tensor(self.N_max, self.dim).uniform_(self.bot, self.top)\n",
    "        idx = torch.randperm(self.N_max)\n",
    "        idx = idx[:N_0]\n",
    "        \n",
    "        T = torch.Tensor(self.N_max, 1).fill_(0.)\n",
    "        X_ics = torch.cat([X_ics[idx], T[idx]], dim=1)\n",
    "        return X_ics\n",
    "    \n",
    "    def func_res(self, X_res):\n",
    "        \"\"\"控制方程右端项\"\"\"\n",
    "        f = X_res[:, :self.dim].mean(axis=1, keepdims=True)\n",
    "        f = (1/self.dim - 1) * torch.cos(f) * torch.exp(- X_res[:, [-1]])\n",
    "        return f\n",
    "    \n",
    "    def func_bcs(self, X_bcs):\n",
    "        \"\"\"边界条件右端项\"\"\"\n",
    "        u_bcs = X_bcs[:, :self.dim].mean(axis=1, keepdims=True)\n",
    "        u_bcs = torch.cos(u_bcs) * torch.exp(- X_bcs[:, [-1]])\n",
    "        return u_bcs\n",
    "    \n",
    "    def func_ics(self, X_ics):\n",
    "        \"\"\"初始条件右端项\"\"\"\n",
    "        u_ics = X_ics[:, :self.dim].mean(axis=1, keepdims=True)\n",
    "        u_ics = torch.cos(u_ics)\n",
    "        return u_ics\n",
    "\n",
    "\n",
    "dataset = Dataset_HeatHD(args.bot_top, args.T, args.dim, args.device)\n",
    "args.dataset = dataset\n",
    "X_res, X_bcs, X_ics, f_res, u_bcs, u_ics = dataset.train_data(args.N_r, args.N_b, args.N_0)\n",
    "print(X_res.shape, X_bcs.shape, X_ics.shape, f_res.shape, u_bcs.shape, u_ics.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35602c4b",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e662bb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Adjusting learning rate of group 0 to 5.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "class Trainer_HeatHD:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.dim = args.dim\n",
    "        self.device = args.device\n",
    "        \n",
    "        self.N_r = args.N_r\n",
    "        self.N_b = args.N_b\n",
    "        self.N_0 = args.N_0\n",
    "        self.dataset = args.dataset\n",
    "        \n",
    "        self.lam_res = args.lam_res\n",
    "        self.lam_bcs = args.lam_bcs\n",
    "        self.lam_ics = args.lam_ics\n",
    "        self.backbone = args.backbone\n",
    "        \n",
    "        self.model_name = self.backbone.__class__.__name__\n",
    "        self.model_path = self.get_model_path()\n",
    "        \n",
    "        self.iters_stage1 = args.iters_stage1\n",
    "        self.iters_stage2 = args.iters_stage2\n",
    "        self.iters_stage3 = args.iters_stage3\n",
    "        self.iters_stage4 = args.iters_stage4\n",
    "        \n",
    "        self.K1 = args.K1\n",
    "        self.K2 = args.K2\n",
    "\n",
    "        # eps = np.finfo(np.float32).eps\n",
    "        self.optimizer_Adam = optim.Adam(self.backbone.parameters(), lr=args.backbone_lr, betas=(0.9, 0.999))\n",
    "        \n",
    "        self.step_size = args.step_size#int( self.iters_stage4 / (np.log(1e-3) / np.log(self.gamma)) )\n",
    "        self.scheduler = ExponentialLR(self.optimizer_Adam, gamma=args.backbone_gamma, verbose=True)\n",
    "        \n",
    "        # subnet (transformer net)\n",
    "        self.transformer = args.transformer\n",
    "        self.optimizer_Adam_transformer = optim.Adam(self.transformer.parameters(), lr=args.subnet_lr)    \n",
    "        self.scheduler_transformer = ExponentialLR(self.optimizer_Adam_transformer, gamma=args.subnet_gamma, verbose=True)\n",
    "        \n",
    "        # data\n",
    "        self.X_res, self.X_bcs, self.X_ics, self.f_res, self.u_bcs, self.u_ics = self.dataset.train_data(self.N_r, self.N_b, self.N_0)\n",
    "        \n",
    "        # Logger\n",
    "        self.logger = {\n",
    "            f\"stage{i + 1}\": {\n",
    "                \"loss\": [],\n",
    "                \"loss_res\": [],\n",
    "                \"loss_bcs\": [],\n",
    "                \"loss_ics\": [],\n",
    "                \"iter\": [],\n",
    "            }\n",
    "            for i in range(4)\n",
    "        }\n",
    "        self.logger_valid = {\n",
    "            f\"stage{i + 1}\": {\n",
    "                \"loss\": [],\n",
    "                \"loss_res\": [],\n",
    "                \"loss_bcs\": [],\n",
    "                \"loss_ics\": [],\n",
    "                \"iter\": [],\n",
    "                \"error\": [],  # relative l2 error\n",
    "                \n",
    "                \"loss_transformer\": [],\n",
    "                \"loss_res_transformer\": [],\n",
    "                \"loss_bcs_transformer\": [],\n",
    "                \"loss_ics_transformer\": [],\n",
    "                \"error_transformer\": [],  # 同一批点 经过transformer变换的rl2\n",
    "                \n",
    "                \"distant\": {\"mean\":[], \"var\":[], \"min\":[], \"max\":[]},\n",
    "            }\n",
    "            for i in range(4)\n",
    "        }\n",
    "        \n",
    "        self.backbone_requires_grad = None  # 用以判断是否要更新该网络\n",
    "        self.transformer_requires_grad = None\n",
    "        \n",
    "        self.k = None\n",
    "        self.k_scale = args.k_scale\n",
    "        \n",
    "    def get_model_path(self):\n",
    "        \"\"\"生成保存模型的路径\"\"\"\n",
    "        if not os.path.exists('models'):\n",
    "            os.mkdir('models')\n",
    "        \n",
    "        path = os.path.join('models', self.model_name)\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        \n",
    "        return path\n",
    "        \n",
    "    def update_train_data(self, update_k=False):\n",
    "        \"\"\"更新采样点\"\"\"\n",
    "        self.X_res, self.X_bcs, self.X_ics, self.f_res, self.u_bcs, self.u_ics = self.dataset.train_data(self.N_r, self.N_b, self.N_0)\n",
    "\n",
    "        if update_k:\n",
    "            self.k = self.update_k(self.X_res, self.f_res)\n",
    "        \n",
    "    def update_k(self, X_res, f_res):\n",
    "        \"\"\"将X_res输入net_r 确定这批X_res的限制残差的系数k\"\"\"\n",
    "        self.backbone.train(False)\n",
    "        \n",
    "        f_res_pred = self.net_r(X_res)\n",
    "        k = (f_res_pred - f_res) ** 2\n",
    "#         k_sort = torch.sort(k.flatten(), dim=0).values\n",
    "#         k_min2 = k_sort[10]  # 不取最小值 取近似近似的\n",
    "#         k_max2 = k_sort[-10]\n",
    "#         k = k / (k_max2 - k_min2)\n",
    "        k = k / (k.max() - k.min())\n",
    "        self.backbone.train()\n",
    "        return k.detach_() * self.k_scale\n",
    "    \n",
    "    def compute_distant_mean_var(self, X1, X2):\n",
    "        d = ((X1 - X2) ** 2).sum(axis=1) ** 0.5\n",
    "        return d.mean().item(), d.var().item(), d.min().item(), d.max().item()\n",
    "            \n",
    "    def set_backbone_requires_grad(self, bl):\n",
    "        \"\"\"设置backbone是否记录梯度\"\"\"\n",
    "        self.backbone_requires_grad = bl\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = bl\n",
    "\n",
    "    def set_transformer_requires_grad(self, bl):\n",
    "        \"\"\"设置transformer是否记录梯度\"\"\"\n",
    "        self.transformer_requires_grad = bl\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad = bl\n",
    "\n",
    "    def net_u(self, X):\n",
    "        return self.backbone(X)\n",
    "\n",
    "    def net_r(self, xt):\n",
    "        xt.requires_grad_(True)\n",
    "        \n",
    "        u = self.net_u(xt)\n",
    "        grad_u = grad(u, xt)[0]\n",
    "        \n",
    "        u_t = grad_u[:, [-1]]\n",
    "        u_xx = torch.zeros(u.shape).to(self.device)\n",
    "        for i in range(self.dim):\n",
    "            u_xx += grad(grad_u[:, [i]], xt)[0][:, [i]]\n",
    "        return u_t - u_xx\n",
    "    \n",
    "    def net_transformer(self, X, k):\n",
    "        X.requires_grad_(False)\n",
    "        return self.transformer(X, k)\n",
    "    \n",
    "    def compute_loss(self, use_transformer=False):\n",
    "        \"\"\"计算loss 可选择是否使用transformer\"\"\"\n",
    "        if use_transformer:\n",
    "            if self.transformer_requires_grad and not self.backbone_requires_grad:\n",
    "                # 只更新transformer 训练点只考虑变换后的X_res_ 用loss_res\n",
    "                \n",
    "                self.X_res_ = self.X_res.clone().detach()\n",
    "                self.X_res_ = self.net_transformer(self.X_res_, self.k)\n",
    "                # clone会新开辟一片内存 detach会从计算图剥离\n",
    "                # 用X_res_.clone().detach()计算f_res\n",
    "                # 可以避免X_res计算图混乱\n",
    "                # 从而保证transformer的网络正常地梯度传播\n",
    "                self.f_res_ = self.dataset.func_res(self.X_res_.clone().detach())  # 细节\n",
    "\n",
    "                f_res_pred_ = self.net_r(self.X_res_)\n",
    "                \n",
    "                self.loss_res = torch.mean((f_res_pred_ - self.f_res_) ** 2)\n",
    "                self.loss = self.lam_res * self.loss_res\n",
    "                \n",
    "            elif self.backbone_requires_grad and not self.transformer_requires_grad:\n",
    "                # 只更新backbone 训练点考虑变换前后的X_res X_res_ X_bcs X_ics 用loss_res和loss_bcs loss_ics\n",
    "                \n",
    "                self.X_res_ = self.X_res.clone().detach()\n",
    "                self.X_res_ = self.net_transformer(self.X_res_, self.k)\n",
    "                self.f_res_ = self.dataset.func_res(self.X_res_.clone().detach())  # 细节\n",
    "                \n",
    "                f_res_pred_ = self.net_r(self.X_res_)\n",
    "                \n",
    "                self.loss_res = torch.mean((f_res_pred_ - self.f_res_) ** 2)\n",
    "\n",
    "                u_bcs_pred = self.net_u(self.X_bcs)\n",
    "                self.loss_bcs = torch.mean((u_bcs_pred - self.u_bcs) ** 2)\n",
    "                \n",
    "                u_ics_pred = self.net_u(self.X_ics)\n",
    "                self.loss_ics = torch.mean((u_ics_pred - self.u_ics) ** 2)\n",
    "\n",
    "                self.loss = self.lam_res * self.loss_res + self.lam_bcs * self.loss_bcs + self.lam_ics * self.loss_ics\n",
    "            else:\n",
    "                raise \"error at compute_loss\"\n",
    "        else:\n",
    "            f_res_pred = self.net_r(self.X_res)\n",
    "            self.loss_res = torch.mean((f_res_pred - self.f_res) ** 2)\n",
    "\n",
    "            u_bcs_pred = self.net_u(self.X_bcs)\n",
    "            self.loss_bcs = torch.mean((u_bcs_pred - self.u_bcs) ** 2)\n",
    "            \n",
    "            u_ics_pred = self.net_u(self.X_ics)\n",
    "            self.loss_ics = torch.mean((u_ics_pred - self.u_ics) ** 2)\n",
    "\n",
    "            self.loss = self.lam_res * self.loss_res + self.lam_bcs * self.loss_bcs + self.lam_ics * self.loss_ics\n",
    "\n",
    "    def log_loss(self):\n",
    "        \"\"\"记录当前loss至logger字典\"\"\"\n",
    "        self.logger[f\"stage{self.current_stage}\"][\"loss\"].append(self.loss.item())\n",
    "        self.logger[f\"stage{self.current_stage}\"][\"loss_res\"].append(self.loss_res.item())\n",
    "        self.logger[f\"stage{self.current_stage}\"][\"loss_bcs\"].append(self.loss_bcs.item())\n",
    "        self.logger[f\"stage{self.current_stage}\"][\"loss_ics\"].append(self.loss_ics.item())\n",
    "        self.logger[f\"stage{self.current_stage}\"][\"iter\"].append(self.iter + 1)\n",
    "\n",
    "    def log_info(self):\n",
    "        \"\"\"保存并打印训练信息\"\"\"\n",
    "        info = f'Stage {self.current_stage} Iter {self.iter+1:5d} Time:{time.time()-self.start_time:.1e} # ' + \\\n",
    "               f'Loss:{self.loss.item():.2e}, Loss_r:{self.loss_res.item():.2e}, Loss_b:{self.loss_bcs.item():.2e}, Loss_0:{self.loss_ics.item():.2e} # ' + \\\n",
    "               f'Valid:{self.valid_loss_value:.2e}, RL2:{self.error_u:.2e}'\n",
    "        with open(\"train_info.txt\", 'a') as f:\n",
    "            f.write(info + '\\n')\n",
    "        print(info)\n",
    "        \n",
    "    def compute_rl2(self, X_res):\n",
    "        \"\"\"计算relative l2 error\"\"\"\n",
    "        u_star = X_res[:, :self.dim].mean(axis=1, keepdims=True)  # 计算解析解\n",
    "        u_star = torch.cos(u_star) * torch.exp(- X_res[:, [-1]])\n",
    "        u_star = u_star.detach().cpu().numpy()\n",
    "        \n",
    "        u_pred = self.net_u(X_res)  # 计算预测解\n",
    "        u_pred = u_pred.detach().cpu().numpy()\n",
    "        \n",
    "        error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)  # rl2 error\n",
    "        return error_u\n",
    "\n",
    "    def valid(self):\n",
    "        \"\"\"验证并保存最优模型\"\"\"\n",
    "        # 计算loss\n",
    "        X_res, X_bcs, X_ics, f_res, u_bcs, u_ics = self.dataset.train_data(1000, 1000, 1000)\n",
    "        k = self.update_k(X_res, f_res)\n",
    "        \n",
    "        self.backbone.eval()\n",
    "        self.transformer.eval()\n",
    "        # 不经过transformer的loss\n",
    "        f_res_pred = self.net_r(X_res)\n",
    "        u_bcs_pred = self.net_u(X_bcs)\n",
    "        u_ics_pred = self.net_u(X_ics)\n",
    "        loss_res = torch.mean((f_res_pred - f_res) ** 2)\n",
    "        loss_bcs = torch.mean((u_bcs_pred - u_bcs) ** 2)\n",
    "        loss_ics = torch.mean((u_ics_pred - u_ics) ** 2)\n",
    "        loss = self.lam_res * loss_res + self.lam_bcs * loss_bcs + self.lam_ics * loss_ics\n",
    "        \n",
    "        # 经过transformer后的内部点的loss\n",
    "        X_res_ = self.net_transformer(X_res, k)\n",
    "        f_res_ = self.dataset.func_res(X_res_.clone().detach())  # 计算相应X_res_的新的f_res_\n",
    "        f_res_pred_transformer = self.net_r(X_res_)\n",
    "        u_bcs_pred_transformer = self.net_u(X_bcs)  # 边界点也计算 对比经过和不经过transformer的两次loss 确保两次loss是基于同一批训练点\n",
    "        u_ics_pred_transformer = self.net_u(X_ics)\n",
    "        loss_res_transformer = torch.mean((f_res_pred_transformer - f_res_) ** 2)\n",
    "        loss_bcs_transformer = torch.mean((u_bcs_pred_transformer - u_bcs) ** 2)\n",
    "        loss_ics_transformer = torch.mean((u_ics_pred_transformer - u_ics) ** 2)\n",
    "        loss_transformer = self.lam_res * loss_res_transformer + self.lam_bcs * loss_bcs_transformer + self.lam_ics * loss_ics_transformer\n",
    "        \n",
    "        # 计算relative l2 error\n",
    "        self.error_u = self.compute_rl2(X_res)\n",
    "        self.error_u_transformer = self.compute_rl2(X_res_)  # 经过transformer变换的内部点 看看其rl2是否会更大 即更难学\n",
    "        \n",
    "        # 计算经过transformer后的变化距离信息\n",
    "        d_mean, d_var, d_min, d_max = self.compute_distant_mean_var(X_res, X_res_)\n",
    "        \n",
    "        self.backbone.train()\n",
    "        self.transformer.train()\n",
    "        \n",
    "        # 记录valid的loss和rl2 error信息\n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"loss\"].append(loss.item())\n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"loss_res\"].append(loss_res.item())\n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"loss_bcs\"].append(loss_bcs.item())\n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"loss_ics\"].append(loss_ics.item())\n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"iter\"].append(self.iter + 1)\n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"error\"].append(self.error_u)\n",
    "        \n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"loss_transformer\"].append(loss_transformer.item())\n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"loss_res_transformer\"].append(loss_res_transformer.item())\n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"loss_bcs_transformer\"].append(loss_bcs_transformer.item())\n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"loss_ics_transformer\"].append(loss_ics_transformer.item())\n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"error_transformer\"].append(self.error_u_transformer)\n",
    "        \n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"distant\"][\"mean\"].append(d_mean)\n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"distant\"][\"var\"].append(d_var)\n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"distant\"][\"min\"].append(d_min)\n",
    "        self.logger_valid[f\"stage{self.current_stage}\"][\"distant\"][\"max\"].append(d_max)\n",
    "        \n",
    "        # 验证模型loss是否最优并选择保存\n",
    "        self.valid_loss_value = loss.item()\n",
    "        is_best = self.valid_loss_value < self.best_loss\n",
    "        if is_best:\n",
    "            self.best_loss = self.valid_loss_value\n",
    "        state = {\n",
    "            'stage': self.current_stage,\n",
    "            'iter': self.iter,\n",
    "            'state_dict': self.backbone.state_dict(),\n",
    "            'state_dict_transformer': self.transformer.state_dict(),\n",
    "            'best_loss': self.best_loss\n",
    "        }\n",
    "        save_model(state, is_best, save_dir=self.model_path)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"DAR 4个Stage 训练\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.best_loss = 1.e10\n",
    "        \n",
    "        # stage1 只训练backbone\n",
    "        self.iter = 0\n",
    "        self.current_stage = 1  # 设置当前stage\n",
    "        self.set_backbone_requires_grad(True)  # 记录backbone的梯度\n",
    "        self.set_transformer_requires_grad(False)  # 不记录transformer的梯度\n",
    "        for _ in range(self.iters_stage1):\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            self.compute_loss(use_transformer=False)\n",
    "            self.loss.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "            self.log_loss()        \n",
    "            \n",
    "            if (self.iter + 1) % 100 == 0:\n",
    "                self.valid()\n",
    "                self.log_info()\n",
    "                self.update_train_data()\n",
    "            if (self.iter + 1) % self.step_size == 0:\n",
    "                self.scheduler.step()\n",
    "            self.iter += 1\n",
    "            \n",
    "            \n",
    "        self.k = self.update_k(self.X_res, self.f_res)\n",
    "            \n",
    "                \n",
    "        # stage2 只训练transformer\n",
    "        self.iter = 0\n",
    "        self.current_stage = 2\n",
    "        self.set_backbone_requires_grad(False)\n",
    "        self.set_transformer_requires_grad(True)\n",
    "        for _ in range(self.iters_stage2):       \n",
    "            self.optimizer_Adam_transformer.zero_grad()\n",
    "            self.compute_loss(use_transformer=True)\n",
    "            self.loss.backward()\n",
    "\n",
    "            for param in self.transformer.parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    param.grad *= -1\n",
    "            self.optimizer_Adam_transformer.step()\n",
    "\n",
    "            self.log_loss()\n",
    "            \n",
    "            if (self.iter + 1) % 10 == 0:\n",
    "                self.valid()\n",
    "                self.log_info()\n",
    "                self.update_train_data(update_k=True)\n",
    "            if (self.iter + 1) % self.step_size == 0:\n",
    "                self.scheduler_transformer.step()\n",
    "            self.iter += 1\n",
    "\n",
    "        # stage3 同时训练backbone和transformer\n",
    "        self.iter = 0\n",
    "        self.current_stage = 3\n",
    "        self.set_backbone_requires_grad(True)\n",
    "        self.set_transformer_requires_grad(True)\n",
    "        for _ in range(self.iters_stage3):\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            self.optimizer_Adam_transformer.zero_grad()\n",
    "            \n",
    "            # 优化K1次backbone 再优化K2次transformer 如此往复\n",
    "            if (self.iter % (self.K1 + self.K2)) < self.K1:\n",
    "                # 使用transformer优化backbone\n",
    "                self.set_backbone_requires_grad(True)\n",
    "                self.set_transformer_requires_grad(False)\n",
    "                self.compute_loss(use_transformer=True)\n",
    "                self.loss.backward()\n",
    "                self.optimizer_Adam.step()\n",
    "            else:\n",
    "                # 优化transformer\n",
    "                self.set_backbone_requires_grad(False)\n",
    "                self.set_transformer_requires_grad(True)\n",
    "                self.compute_loss(use_transformer=True)\n",
    "                self.loss.backward()\n",
    "\n",
    "                for param in self.transformer.parameters():\n",
    "                    if param.requires_grad == True:\n",
    "                        param.grad *= -1\n",
    "                self.optimizer_Adam_transformer.step()\n",
    "            \n",
    "            self.log_loss()\n",
    "\n",
    "            if (self.iter + 1) % 100 == 0:\n",
    "                self.valid()\n",
    "                self.log_info()\n",
    "                self.update_train_data(update_k=True)\n",
    "                \n",
    "            if (self.iter + 1) % self.step_size == 0:\n",
    "                self.scheduler.step()\n",
    "                self.scheduler_transformer.step()\n",
    "            self.iter += 1\n",
    "            \n",
    "        # stage4 只训练backbone\n",
    "        self.iter = 0\n",
    "        self.current_stage = 4\n",
    "        self.set_backbone_requires_grad(True)\n",
    "        self.set_transformer_requires_grad(False)\n",
    "        for _ in range(self.iters_stage4):\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            self.compute_loss(use_transformer=False)\n",
    "            self.loss.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "            self.log_loss()\n",
    "\n",
    "            if (self.iter + 1) % 100 == 0:\n",
    "                self.valid()\n",
    "                self.log_info()\n",
    "                self.update_train_data()\n",
    "\n",
    "            if (self.iter + 1) % self.step_size == 0:\n",
    "                self.scheduler.step()\n",
    "            self.iter += 1\n",
    "            \n",
    "            \n",
    "trainer = Trainer_HeatHD(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6972fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 3 Iter   100 Time:1.6e+01 # Loss:5.17e-02, Loss_r:4.35e-02, Loss_b:4.74e-03, Loss_0:3.45e-03 # Valid:5.53e-02, RL2:1.05e-01\n",
      "Stage 3 Iter   200 Time:3.1e+01 # Loss:1.91e-03, Loss_r:1.31e-03, Loss_b:2.68e-04, Loss_0:3.32e-04 # Valid:2.26e-03, RL2:2.55e-02\n",
      "Stage 3 Iter   300 Time:4.7e+01 # Loss:1.28e-03, Loss_r:8.97e-04, Loss_b:1.72e-04, Loss_0:2.09e-04 # Valid:1.48e-03, RL2:2.02e-02\n",
      "Stage 3 Iter   400 Time:6.1e+01 # Loss:1.00e-03, Loss_r:6.77e-04, Loss_b:1.22e-04, Loss_0:2.05e-04 # Valid:1.14e-03, RL2:1.66e-02\n",
      "Stage 3 Iter   500 Time:8.2e+01 # Loss:7.75e-04, Loss_r:5.21e-04, Loss_b:9.63e-05, Loss_0:1.57e-04 # Valid:9.17e-04, RL2:1.53e-02\n",
      "Stage 3 Iter   600 Time:1.0e+02 # Loss:7.68e-04, Loss_r:7.68e-04, Loss_b:9.63e-05, Loss_0:1.57e-04 # Valid:8.93e-04, RL2:1.46e-02\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4883df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./loss_logger_dict.npy\", trainer.logger)\n",
    "np.save(\"./loss_logger_valid_dict.npy\", trainer.logger_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70328ecd",
   "metadata": {},
   "source": [
    "## 结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7434af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':18})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32348622",
   "metadata": {},
   "source": [
    "### 训练的loss图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2c89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss图\n",
    "loss_logger = np.load(\"./loss_logger_dict.npy\", allow_pickle=True).item()\n",
    "fig = plt.figure(figsize=(24, 5), dpi=64)\n",
    "axes = fig.subplots(nrows=1, ncols=4)\n",
    "\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    logger = loss_logger[f\"stage{i + 1}\"]\n",
    "    \n",
    "    ax.plot(logger[\"iter\"], logger[\"loss\"], label=r\"$Loss$\", linewidth=3)\n",
    "    ax.plot(logger[\"iter\"], logger[\"loss_res\"], label=r\"$Loss_{res}$\", linewidth=2)\n",
    "    ax.plot(logger[\"iter\"], logger[\"loss_bcs\"], label=r\"$Loss_{bcs}$\", linewidth=2)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(f\"Stage {i + 1}\")\n",
    "    ax.grid()\n",
    "\n",
    "lines, labels = fig.axes[-1].get_legend_handles_labels()\n",
    "fig.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.5, -0), ncol=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Loss.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac13fa6b",
   "metadata": {},
   "source": [
    "### 验证的loss图 (包括只经backbone和经过transformer和backbone的loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid的loss图 第一行只经过backbone 第二行经过transformer和backbone\n",
    "# 因为只用backbone来valid 所以valid loss必须是对每个stage都是非增的 否则代码有误\n",
    "# 由于随机采样 stage2的loss在小范围内波动属正常\n",
    "# 第二行的stage2是经过transformer和backbone的 loss上升说明对抗网络是有效的\n",
    "loss_logger_valid = np.load(\"./loss_logger_valid_dict.npy\", allow_pickle=True).item()\n",
    "\n",
    "fig = plt.figure(figsize=(24, 5), dpi=64)\n",
    "axes = fig.subplots(nrows=1, ncols=4)\n",
    "\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    logger = loss_logger_valid[f\"stage{i + 1}\"]\n",
    "    \n",
    "    ax.plot(logger[\"iter\"], logger[\"loss\"], label=r\"$Loss$\", linewidth=3)\n",
    "    ax.plot(logger[\"iter\"], logger[\"loss_res\"], label=r\"$Loss_{res}$\", linewidth=2)\n",
    "    ax.plot(logger[\"iter\"], logger[\"loss_bcs\"], label=r\"$Loss_{bcs}$\", linewidth=2)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(f\"Stage {i + 1}\")\n",
    "    ax.grid()\n",
    "\n",
    "lines, labels = fig.axes[-1].get_legend_handles_labels()\n",
    "fig.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.5, -0), ncol=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('loss_valid.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(24, 5), dpi=64)\n",
    "axes = fig.subplots(nrows=1, ncols=4)\n",
    "\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    logger = loss_logger_valid[f\"stage{i + 1}\"]\n",
    "    \n",
    "    ax.plot(logger[\"iter\"], logger[\"loss_transformer\"], label=r\"$Loss$\", linewidth=3)\n",
    "    ax.plot(logger[\"iter\"], logger[\"loss_res_transformer\"], label=r\"$Loss_{res}$\", linewidth=2)\n",
    "    ax.plot(logger[\"iter\"], logger[\"loss_bcs_transformer\"], label=r\"$Loss_{bcs}$\", linewidth=2)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(f\"Stage {i + 1}\")\n",
    "    ax.grid()\n",
    "\n",
    "lines, labels = fig.axes[-1].get_legend_handles_labels()\n",
    "fig.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.5, -0), ncol=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('loss_valid_transformer.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649be43",
   "metadata": {},
   "source": [
    "### 验证的Relative L2 error图 (包括只经backbone和经过transformer和backbone的rl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07114da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid中记录的relative l2 error记录 第一行只经过backbone 第二行经过transformer和backbone\n",
    "loss_logger_valid = np.load(\"./loss_logger_valid_dict.npy\", allow_pickle=True).item()\n",
    "\n",
    "fig = plt.figure(figsize=(24, 5), dpi=64)\n",
    "axes = fig.subplots(nrows=1, ncols=4)\n",
    "\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    logger = loss_logger_valid[f\"stage{i + 1}\"]\n",
    "    \n",
    "    ax.plot(logger[\"iter\"], logger[\"error\"], label=r\"Backbone Relative error\", color=\"blue\", linewidth=3)\n",
    "    ax.plot(logger[\"iter\"], logger[\"error_transformer\"], label=r\"Transformer+Backbone Relative L2 error\", color=\"red\", linewidth=3)\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(f\"Stage {i + 1}\")\n",
    "    ax.grid()\n",
    "\n",
    "lines, labels = fig.axes[-1].get_legend_handles_labels()\n",
    "fig.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.5, -0), ncol=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Relative_L2_error.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e1a68",
   "metadata": {},
   "source": [
    "### 验证的距离变化信息 d_mean d_var d_min d_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0009081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid中记录的relative l2 error记录 第一行只经过backbone 第二行经过transformer和backbone\n",
    "loss_logger_valid = np.load(\"./loss_logger_valid_dict.npy\", allow_pickle=True).item()\n",
    "\n",
    "fig = plt.figure(figsize=(24, 5), dpi=64)\n",
    "axes = fig.subplots(nrows=1, ncols=4)\n",
    "\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    logger = loss_logger_valid[f\"stage{i + 1}\"]\n",
    "    \n",
    "    ax.plot(logger[\"iter\"], logger[\"distant\"][\"mean\"], label=r\"Mean distant\", color=\"green\", linewidth=3)\n",
    "    ax.plot(logger[\"iter\"], logger[\"distant\"][\"var\"], label=r\"Var distant\", color=\"orange\", linewidth=3)\n",
    "    ax.plot(logger[\"iter\"], logger[\"distant\"][\"min\"], label=r\"Min distant\", color=\"cyan\", linestyle=':', linewidth=3)\n",
    "    ax.plot(logger[\"iter\"], logger[\"distant\"][\"max\"], label=r\"Max distant\", color=\"dodgerblue\", linestyle='--', linewidth=3)\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(f\"Stage {i + 1}\")\n",
    "    ax.grid()\n",
    "\n",
    "lines, labels = fig.axes[-1].get_legend_handles_labels()\n",
    "fig.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.5, -0), ncol=4)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('Relative_L2_error.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b8aaf7",
   "metadata": {},
   "source": [
    "### Relative L2 error (仅考虑内部点）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用最好的模型\n",
    "backbone = ResNet(args.backbone_layers)\n",
    "state_dict = torch.load(f'{trainer.model_path}/best_model.pth')\n",
    "backbone.load_state_dict(state_dict['state_dict'])\n",
    "backbone.eval()\n",
    "print(\"Stage:\\t\", state_dict['stage'])\n",
    "print(\"Iter:\\t\", state_dict['iter'] + 1)\n",
    "print(\"Loss:\\t\", \"{:.2e}\".format(state_dict['best_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_sol(X):\n",
    "    # 解析解\n",
    "    u_star = X[:, :args.dim].mean(axis=1, keepdims=True)\n",
    "    u_star = np.cos(u_star) * np.exp(- X[:, [-1]])\n",
    "    return u_star\n",
    "\n",
    "# 采样内部点\n",
    "dataset = Dataset_HeatHD(args.bot_top, args.T, args.dim, torch.device('cpu'))\n",
    "X_res, _, _, _, _, _ = dataset.train_data(N_r=10000, N_b=0, N_0=0)\n",
    "# 计算u_star u_pred\n",
    "u_star = exact_sol(X_res)\n",
    "u_pred = backbone(X_res)\n",
    "\n",
    "u_star = u_star.detach().numpy()\n",
    "u_pred = u_pred.detach().numpy()\n",
    "#计算L2相对误差\n",
    "error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "with open(\"RL2.txt\", 'w') as f:\n",
    "    f.write(str(error_u))\n",
    "print('Relative L2 error: {:.3e}'.format(error_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319dfb88",
   "metadata": {},
   "source": [
    "### 可视化内部(x1, x2, 0, ..., 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 101\n",
    "dim = args.dim\n",
    "# 取前两维\n",
    "x1 = np.linspace(-1, 1, n)\n",
    "x2 = np.linspace(-1, 1, n)\n",
    "x1, x2 = np.meshgrid(x1, x2)\n",
    "# x3, ..., x50 = 0\n",
    "x12 = np.zeros((n*n, dim))\n",
    "x12[:, [0]] = x1.reshape(-1, 1)\n",
    "x12[:, [1]] = x2.reshape(-1, 1)\n",
    "t = np.ones((x12.shape[0], 1)) * args.T\n",
    "x12 = np.concatenate([x12, t], axis=1)\n",
    "x12 = torch.from_numpy(x12).float()\n",
    "# 解析解和预测解\n",
    "u12_star = exact_sol(x12)\n",
    "u12_pred = backbone(x12)\n",
    "\n",
    "u12_star = u12_star.detach().numpy()\n",
    "u12_pred = u12_pred.detach().numpy()\n",
    "\n",
    "u12_star = u12_star.reshape(x1.shape)\n",
    "u12_pred = u12_pred.reshape(x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f293297",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "axes = fig.subplots(1, 3)\n",
    "\n",
    "cax1 = axes[0].pcolor(x1, x2, u12_star, vmin=u12_star.min(), vmax=u12_star.max(), cmap='jet')\n",
    "fig.colorbar(cax1)\n",
    "axes[0].set_xlabel('$x_1$')\n",
    "axes[0].set_ylabel('$x_2$')\n",
    "axes[0].set_title(r'Reference $u$')\n",
    "axes[0].set_xlim([-1, 1])\n",
    "axes[0].set_ylim([-1, 1])\n",
    "axes[0].set_xticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[0].set_yticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[0].set_aspect(1./axes[0].get_data_ratio())\n",
    "\n",
    "cax2 = axes[1].pcolor(x1, x2, u12_pred, vmin=u12_star.min(), vmax=u12_star.max(), cmap='jet')\n",
    "fig.colorbar(cax2)\n",
    "axes[1].set_xlabel('$x_1$')\n",
    "axes[1].set_ylabel('$x_2$')\n",
    "axes[1].set_title(r'Predicted $u$')\n",
    "axes[1].set_xlim([-1, 1])\n",
    "axes[1].set_ylim([-1, 1])\n",
    "axes[1].set_xticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[1].set_yticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[1].set_aspect(1./axes[1].get_data_ratio())\n",
    "\n",
    "cax3 = axes[2].pcolor(x1, x2, np.abs(u12_star - u12_pred), cmap='jet')\n",
    "fig.colorbar(cax3)\n",
    "axes[2].set_xlabel('$x_1$')\n",
    "axes[2].set_ylabel('$x_2$')\n",
    "axes[2].set_title('Absolute error')\n",
    "axes[2].set_xlim([-1, 1])\n",
    "axes[2].set_ylim([-1, 1])\n",
    "axes[2].set_xticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[2].set_yticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[2].set_aspect(1./axes[2].get_data_ratio())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('HeatHD_result.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e2a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6.5, 5), dpi=100)\n",
    "plot = ax.contourf(x1, x2, np.abs(u12_star - u12_pred), levels=16, cmap='jet')\n",
    "\n",
    "cbar = fig.colorbar(plot)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "cbar.update_ticks()\n",
    "\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_xticks([-1, -0.5, 0, 0.5, 1])\n",
    "ax.set_yticks([-1, -0.5, 0, 0.5, 1])\n",
    "ax.set_title('Absolute error')\n",
    "ax.set_aspect(1./ax.get_data_ratio())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./Absolute_error', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc5562b",
   "metadata": {},
   "source": [
    "## Transformer变换可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f2864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_res\n",
    "trainer.device = torch.device('cpu')\n",
    "trainer.backbone = trainer.backbone.cpu()\n",
    "\n",
    "f_res = trainer.dataset.func_res(x12)\n",
    "f_res_pred = trainer.net_r(x12)\n",
    "loss_res = (f_res - f_res_pred) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.5, 5))\n",
    "ax = fig.subplots()\n",
    "\n",
    "cax = ax.scatter(x12.detach().numpy()[:, 0], x12.detach().numpy()[:, 1], \n",
    "                 c=loss_res.detach().cpu().numpy(), \n",
    "#                  vmin=0, vmax=5e-5,\n",
    "                 cmap=\"jet\", s=80)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_title(r'Residual')\n",
    "ax.set_xlim([-1, 1])\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.set_xticks(np.arange(-1, 1.1, 0.5))\n",
    "ax.set_yticks(np.arange(-1, 1.1, 0.5))\n",
    "ax.set_aspect(1./axes[0].get_data_ratio())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b663924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新取点\n",
    "n = 21\n",
    "dim = args.dim\n",
    "# 取前两维\n",
    "xx1 = np.linspace(-1, 1, n)\n",
    "xx2 = np.linspace(-1, 1, n)\n",
    "xx1, xx2 = np.meshgrid(xx1, xx2)\n",
    "# x3, ..., x50 = 0\n",
    "xx12 = np.zeros((n*n, dim))\n",
    "xx12[:, [0]] = xx1.reshape(-1, 1)\n",
    "xx12[:, [1]] = xx2.reshape(-1, 1)\n",
    "t = np.ones((xx12.shape[0], 1)) * args.T\n",
    "xx12 = np.concatenate([xx12, t], axis=1)\n",
    "xx12 = torch.from_numpy(xx12).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd37916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 变换后的点\n",
    "# model = MLP(args.subnet_layers)  # 保存的loss最低时的transformer\n",
    "# transformer = TransformerNet(model, args.bot_top)\n",
    "\n",
    "# state_dict = torch.load(f'{trainer.model_path}/best_model.pth')\n",
    "# transformer.load_state_dict(state_dict['state_dict_transformer'])\n",
    "# transformer.eval()\n",
    "\n",
    "# xx12_ = transformer(xx12)\n",
    "\n",
    "trainer.transformer = trainer.transformer.cpu()  # 训练最后的transformer\n",
    "f_xx12 = dataset.func_res(xx12)\n",
    "k = trainer.update_k(xx12, f_xx12)\n",
    "xx12_ = trainer.net_transformer(xx12, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(k.detach().cpu().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3535643",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "axes = fig.subplots(1, 2)\n",
    "\n",
    "axes[0].scatter(xx12.detach().numpy()[:, 0], xx12.detach().numpy()[:, 1], s=30, color='b')\n",
    "axes[0].set_xlabel('$x_1$')\n",
    "axes[0].set_ylabel('$x_2$')\n",
    "axes[0].set_title(r'Original distribution')\n",
    "axes[0].set_xlim([-1, 1])\n",
    "axes[0].set_ylim([-1, 1])\n",
    "axes[0].set_xticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[0].set_yticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[0].set_aspect(1./axes[0].get_data_ratio())\n",
    "\n",
    "axes[1].scatter(xx12_.detach().numpy()[:, 0], xx12_.detach().numpy()[:, 1], s=30, color='r')\n",
    "axes[1].set_xlabel('$x_1$')\n",
    "axes[1].set_ylabel('$x_2$')\n",
    "axes[1].set_title('Transformed distribution')\n",
    "axes[1].set_xlim([-1, 1])\n",
    "axes[1].set_ylim([-1, 1])\n",
    "axes[1].set_xticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[1].set_yticks(np.arange(-1, 1.1, 0.5))\n",
    "axes[1].set_aspect(1./axes[1].get_data_ratio())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6fc96a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch39",
   "language": "python",
   "name": "torch39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
